{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguish Your Own Digits (DYOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to write a classifier that distinguishes between the number 3 and number 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the command line run `pip install mnist`. This is a library that will help you bring down the mnist dataset. If you run this from a notebook, you need to put  `!pip install mnist` in a cell by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mnist in /opt/anaconda/lib/python3.7/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda/lib/python3.7/site-packages (from mnist) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc63e376450>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 7776 # You may select anything up to 60,000\n",
    "print(train_labels[image_index]) \n",
    "plt.imshow(train_images[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data to get 3 and 8 out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filter = np.where((train_labels == 3 ) | (train_labels == 8))\n",
    "test_filter = np.where((test_labels == 3) | (test_labels == 8))\n",
    "X_train, y_train = train_images[train_filter], train_labels[train_filter]\n",
    "X_test, y_test = test_images[test_filter], test_labels[test_filter]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the pizel values in the 0 to 1 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.\n",
    "X_test = X_test/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = 1*(y_train==3)\n",
    "y_test = 1*(y_test==3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11982, 28, 28), (1984, 28, 28))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the data to flatten the image pixels into a set of features or co-variates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11982, 784), (1984, 784))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use the following configuration (or similar) to set up your model for training.\n",
    "\n",
    "```pythom\n",
    "class Config:\n",
    "    pass\n",
    "config = Config()\n",
    "config.lr = 0.001\n",
    "config.num_epochs = 200\n",
    "config.bs = 50\n",
    "```\n",
    "\n",
    "Make sure you import everything you need from the kudzu library provided. Fell free to change it if needed, but mention that in your solutions.\n",
    "\n",
    "\n",
    "Now construct a model which has the following layers\n",
    "\n",
    "1. A first affine layer which has 784 inputs and does 100 affine transforms. These are followed by a `Relu`\n",
    "2. A second affine layer which has 100 inputs from the 100 activations of the past layer, and does 100 affine transforms. These are followed by a `Relu`\n",
    "3. A third affine layer which has 100 activations and does 2 affine transformations to create an embedding for visualization. There is no non-linearity here.\n",
    "4. A final \"logistic regression\" which has an affine transform from 2 inputs to 1 output, which is squeezed through a sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a callback class\n",
    "\n",
    "Lets call it `ClfCallback` or classifier-callback. You can build it as a subclass of `AccClallback` or just copy and paste the code. But it needs some new functionality:\n",
    "\n",
    "1. Initialize it to have accuracy arrays \n",
    "\n",
    "```python\n",
    "        self.accuracies = []\n",
    "        self.test_accuracies = []\n",
    "```\n",
    "2. Then at the end of each epoch, calculate the probabilities and hence predictions on both the training set and the test set. Print these out once per epoch. Acumulate these in the above array. This will require you to keep track of all 4 training and test sets. You can edit the `Learner` for this or pass these sets in some kind of data object to the callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for 200, or if needed, some more epochs with a low learning rate. You will find that your losses will not have converged to a completely flat level before gradients start to blow up. The reason for this is the high or low probability instabilities (the $\\frac{dL}{dp}$ gradient) , and also possibly other numerical instabilities..see http://fa.bianp.net/blog/2019/evaluate_logistic/ for more details.\n",
    "\n",
    "Stop the training at 200-300 epochs. When you plot the training and validation accuracies you will see the model is already overfitting: the validation accuracy will have dipped below the training accuracy, and they are diverging.\n",
    "\n",
    "Overfitting happens when your model is quite complex and is fitting to the noise in the training set rather than the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot the results\n",
    "\n",
    "What accuracy do we get? How many False positives and False negatives.\n",
    "\n",
    "Specifically, in the embedding space ( the inputs of the \"logistic regression\") plot the data points by running them forward through the network. Color code them with their actual class and plot the probability contours (these will all be lines in the embedding space as from here on, this is a logistic regression). This plot should allow one to see the points stranded on the \"wrong\" side of the probability 1/2 line. Also plot the predictions against the actual values on a plot of your choice, showing where they are in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. OPTIONAL AND EXTRA CREDIT\n",
    "\n",
    "1. Compare your neural network to a simple logistic regression model.\n",
    "2. Put your notebook on your blog, communicating how you carried out your analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
